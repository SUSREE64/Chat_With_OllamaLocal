# Chat_With_OllamaLocal
Streamlit application that runs a locally running Ollama LLMs for Question answer and chatting. 

Please make sure that you have downloaded Ollama from "https://ollama.com/download/windows" and installed on your windows,
From command prompt / Windows Terminal make sure you pull few models of your interest with thier respective names. For
example if you want to download a model " ollama 3.3." you need to give a command "ollama pull llama3.3" . This will load the model to windows local and this app can find that and can show in the drop down menu.

