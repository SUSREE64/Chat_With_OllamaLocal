# Chat_With_OllamaLocal
Streamlit application that runs a locally running Ollama LLMs for Question answer and chatting. 

Please make sure that you have downloaded Ollama from "https://ollama.com/download/windows" and installed on your windows,
From command prompt / Windows Terminal make sure you pull few models of your interest with thier respective names. For
example if you want to download a model " ollama 3.3." you need to give a command "ollama pull llama3.3" . This will load the model to windows local and this app can find that and can show in the drop down menu.

To run this application. 

1. Clone this repository to your local folder
2. Create a python virtual environment. 
3. Activate the environment
4. From command prompt run "pip install -r requirement.txt"
5. After the modules installed  from Terminal / command prompt  "streamlit run ollama_chat.py"
6. The streamlit opens up the interface on local browser "localhost:8501" 

